{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354a957e",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4689ab0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we will train a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e86c5",
   "metadata": {},
   "source": [
    "We begin with some initial classes and functions to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "# optional parameters allow target data to be shifted and scaled\n",
    "    def __init__(self, target, data):\n",
    "        self.label = target.astype(np.float32)\n",
    "# assumes that data is prepared in correct shape beforehand\n",
    "        self.input = data\n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index].astype(np.float32), self.label[index]\n",
    "    \n",
    "def randomsplitdata(target_input_fn,training_fraction,maxrows=99999999):\n",
    "    f=open(target_input_fn)\n",
    "    lsplit=f.readline().split()\n",
    "    f.close()\n",
    "        \n",
    "    dft=pd.read_csv(target_input_fn,sep=' ',header=None,usecols=range(0,3),nrows=maxrows)\n",
    "    targetdata=dft.to_numpy()\n",
    "\n",
    "    dfi=pd.read_csv(target_input_fn,sep=' ',header=None,usecols=range(4,len(lsplit)),nrows=maxrows)\n",
    "    inputdata=np.reshape(dfi.to_numpy(),(len(dfi),1,-1))\n",
    "    \n",
    "    xlist=[*range(4,len(lsplit),3)]\n",
    "    ylist=[*range(5,len(lsplit),3)]\n",
    "    zlist=[*range(6,len(lsplit),3)]\n",
    "    \n",
    "    dfixyz=dfi[xlist+ylist+zlist]\n",
    "    inpxyzdata=np.reshape(dfixyz.to_numpy(),(len(dfi),3,-1)) \n",
    "    \n",
    "    print(len(targetdata))\n",
    "    \n",
    "    flag=np.zeros(len(targetdata),dtype=int)\n",
    "    while np.average(flag)<training_fraction:\n",
    "        flag[random.randint(0,len(targetdata)-1)]=1\n",
    "    \n",
    "    target_training=targetdata[np.nonzero(flag)].copy()\n",
    "    target_validation=targetdata[np.nonzero(1-flag)].copy()\n",
    "    input_training=inputdata[np.nonzero(flag)].copy()\n",
    "    input_validation=inputdata[np.nonzero(1-flag)].copy()\n",
    "    inputxyz_training=inpxyzdata[np.nonzero(flag)].copy()\n",
    "    inputxyz_validation=inpxyzdata[np.nonzero(1-flag)].copy()\n",
    "        \n",
    "    return target_training,input_training,inputxyz_training, \\\n",
    "           target_validation,input_validation,inputxyz_validation\n",
    "\n",
    "def get_loaders(target_input_fn,training_fraction,batch_size=128,maxrows=99999999):\n",
    "    [ttarget,tinput,tinpxyz,vtarget,vinput,vinpxyz]=randomsplitdata(target_input_fn,training_fraction,maxrows) \n",
    "    \n",
    "    train_set=Dataset(ttarget,tinput)\n",
    "    validation_set=Dataset(vtarget,vinput)\n",
    "    trainxyz_set=Dataset(ttarget,tinpxyz)\n",
    "    valixyz_set=Dataset(vtarget,vinpxyz)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=1)\n",
    "    \n",
    "    trainxyz_loader = torch.utils.data.DataLoader(trainxyz_set, batch_size=batch_size, shuffle=True)\n",
    "    valixyz_loader = torch.utils.data.DataLoader(valixyz_set, batch_size=1)\n",
    "    \n",
    "    return train_loader,validation_loader,trainxyz_loader,valixyz_loader        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93f287",
   "metadata": {},
   "source": [
    "Now we setup some training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(m,loss_fn,opt,loader,xscale=1.0,yscale=1.0,zscale=1.0,klfactor=0.0):\n",
    "    klloss=nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    loss_sum = 0.0\n",
    "    for input, label in loader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        output = m(input)              # this is where the model is evaluated\n",
    "        \n",
    "        loss =  loss_fn(output[:,0], label[:,0])*xscale  # model loss for x\n",
    "        loss += loss_fn(output[:,1], label[:,1])*yscale  # model loss for y\n",
    "        loss += loss_fn(output[:,2], label[:,2])*zscale  # model loss for z\n",
    "        \n",
    "        loss_sum += loss.item()        # accumulate MSE loss\n",
    "            \n",
    "        if (klfactor>0):\n",
    "            loss=loss+klfactor*klloss(output[:,0],label[:,0])\n",
    "            \n",
    "        loss.backward()                # this calculates the back-propagated loss\n",
    "        opt.step()                     # this carries out the gradient descent\n",
    "    \n",
    "    return loss_sum / len(loader)      # Note: KL loss is not included in reported loss\n",
    "\n",
    "def validate(m,loss_fn,loader,xscale=1.0,yscale=1.0,zscale=1.0):\n",
    "    loss_sum = 0.0\n",
    "    for input, label in loader:\n",
    "        with torch.no_grad():\n",
    "            output = m(input)\n",
    "        \n",
    "        loss =  loss_fn(output[:,0], label[:,0])*xscale  # model loss for x\n",
    "        loss += loss_fn(output[:,1], label[:,1])*yscale  # model loss for y\n",
    "        loss += loss_fn(output[:,2], label[:,2])*zscale  # model loss for z\n",
    "\n",
    "        #loss = loss_fn(output, label)\n",
    "        loss_sum += loss.item()\n",
    "    return loss_sum / len(loader)\n",
    "\n",
    "def do_training(m,opt,tloader,vloader,epochs,output,xscale=1.0,yscale=1.0,zscale=1.0,klfactor=0.0):\n",
    "    # use MSE loss fucntion\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    tloss=np.zeros(epochs)\n",
    "    vloss=np.zeros(epochs)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        tloss[i] = train(m,loss_fn,opt,tloader,xscale,yscale,zscale,klfactor)\n",
    "        vloss[i] = validate(m,loss_fn,vloader,xscale,yscale,zscale)\n",
    "        if (output):\n",
    "            print (i, tloss[i], vloss[i])\n",
    "            \n",
    "    return tloss,vloss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807f303",
   "metadata": {},
   "source": [
    "We also define some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05dc26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def plot_progress(epochs,tloss,vloss):\n",
    "    plt.rcParams[\"figure.figsize\"]=(6,4)\n",
    "    epoch_index=np.arange(epochs)\n",
    "    plt.plot(epoch_index,np.log(tloss),color='r',label='training')\n",
    "    plt.plot(epoch_index,np.log(vloss),color='b',label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation(loader,m):    \n",
    "    targetx=[]\n",
    "    targety=[]\n",
    "    targetz=[]\n",
    "    \n",
    "    predictionx=[]\n",
    "    predictiony=[]\n",
    "    predictionz=[]\n",
    "\n",
    "    for input, label in loader:        \n",
    "        with torch.no_grad():\n",
    "            output = m(input)\n",
    "                 \n",
    "        targetx+=[label[0,0].item()]\n",
    "        targety+=[label[0,1].item()]\n",
    "        targetz+=[label[0,2].item()]\n",
    "        \n",
    "        predictionx+=[output[0,0].item()]\n",
    "        predictiony+=[output[0,1].item()]\n",
    "        predictionz+=[output[0,2].item()]\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"]=(12,3)\n",
    "\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    \n",
    "    minval=np.min(targetx)\n",
    "    maxval=np.max(targetx)\n",
    "    lin=np.linspace(minval-0.1*(maxval-minval),maxval+0.1*(maxval-minval),num=100)\n",
    "    \n",
    "    ax[0].plot(lin,lin,'k',linewidth=2)\n",
    "    ax[0].plot(targetx,predictionx,'ro',markersize=2)\n",
    "    ax[0].set(xlabel='target x [nm]', ylabel=\"prediction [nm]\")\n",
    "    \n",
    "    minval=np.min(targety)\n",
    "    maxval=np.max(targety)\n",
    "    lin=np.linspace(minval-0.1*(maxval-minval),maxval+0.1*(maxval-minval),num=100)\n",
    "\n",
    "    ax[1].plot(lin,lin,'k',linewidth=2)\n",
    "    ax[1].plot(targety,predictiony,'ro',markersize=2)\n",
    "    ax[1].set(xlabel='target y [nm]', ylabel=\"\")\n",
    "\n",
    "    minval=np.min(targetz)\n",
    "    maxval=np.max(targetz)\n",
    "    lin=np.linspace(minval-0.1*(maxval-minval),maxval+0.1*(maxval-minval),num=100)\n",
    "\n",
    "    ax[2].plot(lin,lin,'k',linewidth=2)\n",
    "    ax[2].plot(targetz,predictionz,'ro',markersize=2)    \n",
    "    ax[2].set(xlabel='target z [nm]', ylabel=\"\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def linear_regression_output(x,y,txt):\n",
    "    x=np.reshape(np.array(x),(-1,1))\n",
    "    y=np.reshape(np.array(y),(-1,1))\n",
    "    linmodel=LinearRegression().fit(x,y)\n",
    "    r2=linmodel.score(x,y)\n",
    "    mval=linmodel.coef_[0]\n",
    "    nval=linmodel.intercept_\n",
    "    print(f'{txt}: r2 {r2} slope {mval[0]} offset {nval[0]}')\n",
    "    \n",
    "def linear_regression(loader,m):    \n",
    "    targetx=[]\n",
    "    targety=[]\n",
    "    targetz=[]\n",
    "    \n",
    "    predictionx=[]\n",
    "    predictiony=[]\n",
    "    predictionz=[]\n",
    "\n",
    "    for input, label in loader:        \n",
    "        with torch.no_grad():\n",
    "            output = m(input)\n",
    "                 \n",
    "        targetx+=[label[0,0].item()]\n",
    "        targety+=[label[0,1].item()]\n",
    "        targetz+=[label[0,2].item()]\n",
    "        \n",
    "        predictionx+=[output[0,0].item()]\n",
    "        predictiony+=[output[0,1].item()]\n",
    "        predictionz+=[output[0,2].item()]\n",
    "    \n",
    "    linear_regression_output(targetx,predictionx,\"x\")\n",
    "    linear_regression_output(targety,predictiony,\"y\")\n",
    "    linear_regression_output(targetz,predictionz,\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484abb0f",
   "metadata": {},
   "source": [
    "Next we define a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ce085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelFC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelFC, self).__init__()\n",
    "        # define layers to be used\n",
    "        self.fc_1 = nn.Linear(30,256)\n",
    "        self.fc_2 = nn.Linear(256,256)\n",
    "        self.fc_3 = nn.Linear(256,256)\n",
    "        self.fc_4 = nn.Linear(256,256)\n",
    "        self.fc_5 = nn.Linear(256, 64)         \n",
    "        self.fc_f = nn.Linear(64, 3)           \n",
    "    def forward(self, x):\n",
    "        # back-propagation is done automatically\n",
    "        x = x.reshape(len(x),-1)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x)) \n",
    "        x = F.relu(self.fc_3(x))\n",
    "        x = F.relu(self.fc_4(x))\n",
    "        x = F.relu(self.fc_5(x))\n",
    "        x = self.fc_f(x)         \n",
    "        return x\n",
    "    def initialize_weights(self, m):\n",
    "        # initialization of weights, setting them to zero is not good\n",
    "        if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            \n",
    "class Model1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1D, self).__init__()\n",
    "        # define layers to be used\n",
    "        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv_3 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv_f = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        # dimensional flattening\n",
    "        self.flatten = nn.Flatten(start_dim=1) \n",
    "        # fully connected layers\n",
    "        self.fc_1 = nn.Linear(480,256)\n",
    "        self.fc_2 = nn.Linear(256,64)\n",
    "        self.fc_f = nn.Linear(64, 3)           \n",
    "    def forward(self, x):\n",
    "        # back-propagation is done automatically\n",
    "        x = self.conv_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = F.relu(self.conv_4(x))\n",
    "        x = F.relu(self.conv_f(x))\n",
    "        x = self.flatten(x)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x)) \n",
    "        x = self.fc_f(x)       \n",
    "        #print(x.size())\n",
    "        return x\n",
    "    def initialize_weights(self, m):\n",
    "        # initialization of weights, setting them to zero is not good\n",
    "        if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            \n",
    "class Model1D3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1D3, self).__init__()\n",
    "        # define layers to be used\n",
    "        self.conv_1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv_3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv_f = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        # dimensional flattening\n",
    "        self.flatten = nn.Flatten(start_dim=1) \n",
    "        # fully connected layers\n",
    "        self.fc_1 = nn.Linear(640,128)\n",
    "        self.fc_2 = nn.Linear(128,32)\n",
    "        self.fc_f = nn.Linear(32, 3)           \n",
    "    def forward(self, x):\n",
    "        # back-propagation is done automatically\n",
    "        x = self.conv_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = F.relu(self.conv_4(x))\n",
    "        x = F.relu(self.conv_f(x))\n",
    "        x = self.flatten(x)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x)) \n",
    "        x = self.fc_f(x)       \n",
    "        #print(x.size())\n",
    "        return x\n",
    "    def initialize_weights(self, m):\n",
    "        # initialization of weights, setting them to zero is not good\n",
    "        if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "            nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4eb241",
   "metadata": {},
   "source": [
    "Now we are ready to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8156e",
   "metadata": {},
   "source": [
    "The following code shows how to train a fully-connected model with the data from the short PDB list. \n",
    "\n",
    "This can be SKIPPED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afde48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tloader,vloader,txyzloader,vxyzloader]=get_loaders('../generatingfeatures/first_local_cai_aa_capm2opmnpmcpm.dat',0.8) \n",
    "\n",
    "m = ModelFC()\n",
    "m.apply(m.initialize_weights)\n",
    "m.zero_grad()\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "\n",
    "epochs=20\n",
    "showoutput=True\n",
    "\n",
    "[tloss,vloss]=do_training(m,opt,tloader,vloader,epochs,showoutput,klfactor=0.5,xscale=2.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vloader,m)\n",
    "linear_regression(vloader,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e67f06",
   "metadata": {},
   "source": [
    "The following code shows how to train a simple 1D convolutional model with the data from the short PDB list. \n",
    "\n",
    "This can be SKIPPED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tloader,vloader,txyzloader,vxyzloader]= \\\n",
    "    get_loaders('../generatingfeatures/first_local_cai_aa_capm2opmnpmcpm.dat',0.8,batch_size=512) \n",
    "\n",
    "m = ModelD()\n",
    "m.apply(m.initialize_weights)\n",
    "m.zero_grad()\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.002, weight_decay=0.00001)\n",
    "\n",
    "epochs=20\n",
    "showoutput=True\n",
    "\n",
    "[tloss,vloss]=do_training(m,opt,tloader,vloader,epochs,showoutput,klfactor=0.8,xscale=2.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vloader,m)\n",
    "linear_regression(vloader,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52305f5a",
   "metadata": {},
   "source": [
    "RUN the following code to start training the 1D3 model based on the longer PDB list. This is what we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tloader,vloader,txyzloader,vxyzloader]= \\\n",
    "    get_loaders('../generatingfeatures/longer_local_cai_aa_capm2opmnpmcpm.dat',0.8,\\\n",
    "                batch_size=512) \n",
    "\n",
    "m = Model1D3()\n",
    "m.apply(m.initialize_weights)\n",
    "m.zero_grad()\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.001, weight_decay=0.000001)\n",
    "\n",
    "epochs=30\n",
    "showoutput=True\n",
    "\n",
    "[tloss,vloss]=do_training(m,opt,txyzloader,vxyzloader,epochs,showoutput,klfactor=2.0,xscale=10.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vxyzloader,m)\n",
    "linear_regression(vxyzloader,m)\n",
    "\n",
    "torch.save(m.state_dict(),\"ca_1d3_predict.dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15530760",
   "metadata": {},
   "source": [
    "We can continue with the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "[tloss,vloss]=do_training(m,opt,txyzloader,vxyzloader,epochs,showoutput,klfactor=2.0,xscale=10.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vxyzloader,m)\n",
    "linear_regression(vxyzloader,m)\n",
    "\n",
    "torch.save(m.state_dict(),\"ca_1d3_predict_2.dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3c634",
   "metadata": {},
   "source": [
    "and more training after loading the model again using a smaller learning rate.\n",
    "\n",
    "The following code can be run after stopping and restarting the notebook because it starts from a saved set of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Model1D3()\n",
    "m.load_state_dict(torch.load('ca_1d3_predict_2.dict'))\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.0001, weight_decay=0.000001)\n",
    "\n",
    "[tloader,vloader,txyzloader,vxyzloader]= \\\n",
    "    get_loaders('../generatingfeatures/longer_local_cai_aa_capm2opmnpmcpm.dat',0.8,\\\n",
    "                batch_size=512) \n",
    "\n",
    "showoutput=True\n",
    "epochs=10\n",
    "\n",
    "[tloss,vloss]=do_training(m,opt,txyzloader,vxyzloader,epochs,showoutput,klfactor=0.5,xscale=10.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vxyzloader,m)\n",
    "linear_regression(vxyzloader,m)\n",
    "\n",
    "torch.save(m.state_dict(),\"ca_1d3_predict_3.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90335e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
