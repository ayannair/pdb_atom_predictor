{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5650fdcb",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25474a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we will train a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6239223",
   "metadata": {},
   "source": [
    "We begin with some initial classes and functions to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d561606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "# optional parameters allow target data to be shifted and scaled\n",
    "    def __init__(self, target, data):\n",
    "        self.label = target.astype(np.float32)\n",
    "# assumes that data is prepared in correct shape beforehand\n",
    "        self.input = data\n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index].astype(np.float32), self.label[index]\n",
    "    \n",
    "def randomsplitdata(target_input_fn,training_fraction,maxrows=99999999):\n",
    "    f=open(target_input_fn)\n",
    "    lsplit=f.readline().split()\n",
    "    f.close()\n",
    "        \n",
    "    dft=pd.read_csv(target_input_fn,sep=' ',header=None,usecols=range(0,3),nrows=maxrows)\n",
    "    targetdata=dft.to_numpy()\n",
    "\n",
    "    dfi=pd.read_csv(target_input_fn,sep=' ',header=None,usecols=range(4,len(lsplit)),nrows=maxrows)\n",
    "    inputdata=np.reshape(dfi.to_numpy(),(len(dfi),1,-1))\n",
    "    \n",
    "    xlist=[*range(4,len(lsplit),3)]\n",
    "    ylist=[*range(5,len(lsplit),3)]\n",
    "    zlist=[*range(6,len(lsplit),3)]\n",
    "    \n",
    "    dfixyz=dfi[xlist+ylist+zlist]\n",
    "    inpxyzdata=np.reshape(dfixyz.to_numpy(),(len(dfi),3,-1)) \n",
    "    \n",
    "    print(len(targetdata))\n",
    "    \n",
    "    flag=np.zeros(len(targetdata),dtype=int)\n",
    "    while np.average(flag)<training_fraction:\n",
    "        flag[random.randint(0,len(targetdata)-1)]=1\n",
    "    \n",
    "    target_training=targetdata[np.nonzero(flag)].copy()\n",
    "    target_validation=targetdata[np.nonzero(1-flag)].copy()\n",
    "    input_training=inputdata[np.nonzero(flag)].copy()\n",
    "    input_validation=inputdata[np.nonzero(1-flag)].copy()\n",
    "    inputxyz_training=inpxyzdata[np.nonzero(flag)].copy()\n",
    "    inputxyz_validation=inpxyzdata[np.nonzero(1-flag)].copy()\n",
    "        \n",
    "    return target_training,input_training,inputxyz_training, \\\n",
    "           target_validation,input_validation,inputxyz_validation\n",
    "\n",
    "def get_loaders(target_input_fn,training_fraction,batch_size=128,maxrows=99999999):\n",
    "    [ttarget,tinput,tinpxyz,vtarget,vinput,vinpxyz]=randomsplitdata(target_input_fn,training_fraction,maxrows) \n",
    "    \n",
    "    train_set=Dataset(ttarget,tinput)\n",
    "    validation_set=Dataset(vtarget,vinput)\n",
    "    trainxyz_set=Dataset(ttarget,tinpxyz)\n",
    "    valixyz_set=Dataset(vtarget,vinpxyz)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=1)\n",
    "    \n",
    "    trainxyz_loader = torch.utils.data.DataLoader(trainxyz_set, batch_size=batch_size, shuffle=True)\n",
    "    valixyz_loader = torch.utils.data.DataLoader(valixyz_set, batch_size=1)\n",
    "    \n",
    "    return train_loader,validation_loader,trainxyz_loader,valixyz_loader        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1f54a",
   "metadata": {},
   "source": [
    "Now we setup some training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de741f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(m,loss_fn,opt,loader,xscale=1.0,yscale=1.0,zscale=1.0,klfactor=0.0):\n",
    "    klloss=nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    loss_sum = 0.0\n",
    "    for input, label in loader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        output = m(input)              # this is where the model is evaluated\n",
    "        \n",
    "        loss =  loss_fn(output[:,0], label[:,0])*xscale  # model loss for x\n",
    "        loss += loss_fn(output[:,1], label[:,1])*yscale  # model loss for y\n",
    "        loss += loss_fn(output[:,2], label[:,2])*zscale  # model loss for z\n",
    "        \n",
    "        loss_sum += loss.item()        # accumulate MSE loss\n",
    "            \n",
    "        if (klfactor>0):\n",
    "            loss=loss+klfactor*klloss(output[:,0],label[:,0])\n",
    "            \n",
    "        loss.backward()                # this calculates the back-propagated loss\n",
    "        opt.step()                     # this carries out the gradient descent\n",
    "    \n",
    "    return loss_sum / len(loader)      # Note: KL loss is not included in reported loss\n",
    "\n",
    "def validate(m,loss_fn,loader,xscale=1.0,yscale=1.0,zscale=1.0):\n",
    "    loss_sum = 0.0\n",
    "    for input, label in loader:\n",
    "        with torch.no_grad():\n",
    "            output = m(input)\n",
    "        \n",
    "        loss =  loss_fn(output[:,0], label[:,0])*xscale  # model loss for x\n",
    "        loss += loss_fn(output[:,1], label[:,1])*yscale  # model loss for y\n",
    "        loss += loss_fn(output[:,2], label[:,2])*zscale  # model loss for z\n",
    "\n",
    "        #loss = loss_fn(output, label)\n",
    "        loss_sum += loss.item()\n",
    "    return loss_sum / len(loader)\n",
    "\n",
    "def do_training(m,opt,tloader,vloader,epochs,output,xscale=1.0,yscale=1.0,zscale=1.0,klfactor=0.0):\n",
    "    # use MSE loss fucntion\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    tloss=np.zeros(epochs)\n",
    "    vloss=np.zeros(epochs)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        tloss[i] = train(m,loss_fn,opt,tloader,xscale,yscale,zscale,klfactor)\n",
    "        vloss[i] = validate(m,loss_fn,vloader,xscale,yscale,zscale)\n",
    "        if (output):\n",
    "            print (i, tloss[i], vloss[i])\n",
    "            \n",
    "    return tloss,vloss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60f79c",
   "metadata": {},
   "source": [
    "We also define some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640721e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def plot_progress(epochs,tloss,vloss):\n",
    "    plt.rcParams[\"figure.figsize\"]=(6,4)\n",
    "    epoch_index=np.arange(epochs)\n",
    "    plt.plot(epoch_index,np.log(tloss),color='r',label='training')\n",
    "    plt.plot(epoch_index,np.log(vloss),color='b',label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation(loader,m):    \n",
    "    targetx=[]\n",
    "    targety=[]\n",
    "    targetz=[]\n",
    "    \n",
    "    predictionx=[]\n",
    "    predictiony=[]\n",
    "    predictionz=[]\n",
    "\n",
    "    for input, label in loader:        \n",
    "        with torch.no_grad():\n",
    "            output = m(input)\n",
    "                 \n",
    "        targetx+=[label[0,0].item()]\n",
    "        targety+=[label[0,1].item()]\n",
    "        targetz+=[label[0,2].item()]\n",
    "        \n",
    "        predictionx+=[output[0,0].item()]\n",
    "        predictiony+=[output[0,1].item()]\n",
    "        predictionz+=[output[0,2].item()]\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"]=(12,3)\n",
    "\n",
    "    fig,ax = plt.subplots(1,3)\n",
    "    \n",
    "    minval=np.min(targetx)\n",
    "    maxval=np.max(targetx)\n",
    "    lin=np.linspace(minval-0.1*(maxval-minval),maxval+0.1*(maxval-minval),num=100)\n",
    "    \n",
    "    ax[0].plot(lin,lin,'k',linewidth=2)\n",
    "    ax[0].plot(targetx,predictionx,'ro',markersize=2)\n",
    "    ax[0].set(xlabel='target x [nm]', ylabel=\"prediction [nm]\")\n",
    "    \n",
    "    minval=np.min(targety)\n",
    "    maxval=np.max(targety)\n",
    "    lin=np.linspace(minval-0.1*(maxval-minval),maxval+0.1*(maxval-minval),num=100)\n",
    "\n",
    "    ax[1].plot(lin,lin,'k',linewidth=2)\n",
    "    ax[1].plot(targety,predictiony,'ro',markersize=2)\n",
    "    ax[1].set(xlabel='target y [nm]', ylabel=\"\")\n",
    "\n",
    "    minval=np.min(targetz)\n",
    "    maxval=np.max(targetz)\n",
    "    lin=np.linspace(minval-0.1*(maxval-minval),maxval+0.1*(maxval-minval),num=100)\n",
    "\n",
    "    ax[2].plot(lin,lin,'k',linewidth=2)\n",
    "    ax[2].plot(targetz,predictionz,'ro',markersize=2)    \n",
    "    ax[2].set(xlabel='target z [nm]', ylabel=\"\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def linear_regression_output(x,y,txt):\n",
    "    x=np.reshape(np.array(x),(-1,1))\n",
    "    y=np.reshape(np.array(y),(-1,1))\n",
    "    linmodel=LinearRegression().fit(x,y)\n",
    "    r2=linmodel.score(x,y)\n",
    "    mval=linmodel.coef_[0]\n",
    "    nval=linmodel.intercept_\n",
    "    print(f'{txt}: r2 {r2} slope {mval[0]} offset {nval[0]}')\n",
    "    \n",
    "def linear_regression(loader,m):    \n",
    "    targetx=[]\n",
    "    targety=[]\n",
    "    targetz=[]\n",
    "    \n",
    "    predictionx=[]\n",
    "    predictiony=[]\n",
    "    predictionz=[]\n",
    "\n",
    "    for input, label in loader:        \n",
    "        with torch.no_grad():\n",
    "            output = m(input)\n",
    "                 \n",
    "        targetx+=[label[0,0].item()]\n",
    "        targety+=[label[0,1].item()]\n",
    "        targetz+=[label[0,2].item()]\n",
    "        \n",
    "        predictionx+=[output[0,0].item()]\n",
    "        predictiony+=[output[0,1].item()]\n",
    "        predictionz+=[output[0,2].item()]\n",
    "    \n",
    "    linear_regression_output(targetx,predictionx,\"x\")\n",
    "    linear_regression_output(targety,predictiony,\"y\")\n",
    "    linear_regression_output(targetz,predictionz,\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff172fe",
   "metadata": {},
   "source": [
    "Next we define a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f191307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelFC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelFC, self).__init__()\n",
    "        # define layers to be used\n",
    "        self.fc_1 = nn.Linear(30,256)\n",
    "        self.fc_2 = nn.Linear(256,256)\n",
    "        self.fc_3 = nn.Linear(256,256)\n",
    "        self.fc_4 = nn.Linear(256,256)\n",
    "        self.fc_5 = nn.Linear(256, 64)         \n",
    "        self.fc_f = nn.Linear(64, 3)           \n",
    "    def forward(self, x):\n",
    "        # back-propagation is done automatically\n",
    "        x = x.reshape(len(x),-1)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x)) \n",
    "        x = F.relu(self.fc_3(x))\n",
    "        x = F.relu(self.fc_4(x))\n",
    "        x = F.relu(self.fc_5(x))\n",
    "        x = self.fc_f(x)         \n",
    "        return x\n",
    "    def initialize_weights(self, m):\n",
    "        # initialization of weights, setting them to zero is not good\n",
    "        if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            \n",
    "class Model1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1D, self).__init__()\n",
    "        # define layers to be used\n",
    "        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv_3 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv_f = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        # dimensional flattening\n",
    "        self.flatten = nn.Flatten(start_dim=1) \n",
    "        # fully connected layers\n",
    "        self.fc_1 = nn.Linear(480,256)\n",
    "        self.fc_2 = nn.Linear(256,64)\n",
    "        self.fc_f = nn.Linear(64, 3)           \n",
    "    def forward(self, x):\n",
    "        # back-propagation is done automatically\n",
    "        x = self.conv_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = F.relu(self.conv_4(x))\n",
    "        x = F.relu(self.conv_f(x))\n",
    "        x = self.flatten(x)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x)) \n",
    "        x = self.fc_f(x)       \n",
    "        #print(x.size())\n",
    "        return x\n",
    "    def initialize_weights(self, m):\n",
    "        # initialization of weights, setting them to zero is not good\n",
    "        if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            \n",
    "class Model1D3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1D3, self).__init__()\n",
    "        # define layers to be used\n",
    "        self.conv_1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv_3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv_f = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        # dimensional flattening\n",
    "        self.flatten = nn.Flatten(start_dim=1) \n",
    "        # fully connected layers\n",
    "        self.fc_1 = nn.Linear(640,128)\n",
    "        self.fc_2 = nn.Linear(128,32)\n",
    "        self.fc_f = nn.Linear(32, 3)           \n",
    "    def forward(self, x):\n",
    "        # back-propagation is done automatically\n",
    "        x = self.conv_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = F.relu(self.conv_4(x))\n",
    "        x = F.relu(self.conv_f(x))\n",
    "        x = self.flatten(x)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x)) \n",
    "        x = self.fc_f(x)       \n",
    "        #print(x.size())\n",
    "        return x\n",
    "    def initialize_weights(self, m):\n",
    "        # initialization of weights, setting them to zero is not good\n",
    "        if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "            nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f7507",
   "metadata": {},
   "source": [
    "Now we are ready to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de071ce9",
   "metadata": {},
   "source": [
    "The following code shows how to train a fully-connected model with the data from the short PDB list. \n",
    "\n",
    "This can be SKIPPED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tloader,vloader,txyzloader,vxyzloader]=get_loaders('../generatingfeatures/first_local_cai_aa_capm2opmnpmcpm.dat',0.8) \n",
    "\n",
    "m = ModelFC()\n",
    "m.apply(m.initialize_weights)\n",
    "m.zero_grad()\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "\n",
    "epochs=20\n",
    "showoutput=True\n",
    "\n",
    "[tloss,vloss]=do_training(m,opt,tloader,vloader,epochs,showoutput,klfactor=0.5,xscale=2.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vloader,m)\n",
    "linear_regression(vloader,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd32a5",
   "metadata": {},
   "source": [
    "The following code shows how to train a simple 1D convolutional model with the data from the short PDB list. \n",
    "\n",
    "This can be SKIPPED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tloader,vloader,txyzloader,vxyzloader]= \\\n",
    "    get_loaders('../generatingfeatures/first_local_cai_aa_capm2opmnpmcpm.dat',0.8,batch_size=512) \n",
    "\n",
    "m = ModelD()\n",
    "m.apply(m.initialize_weights)\n",
    "m.zero_grad()\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.002, weight_decay=0.00001)\n",
    "\n",
    "epochs=20\n",
    "showoutput=True\n",
    "\n",
    "[tloss,vloss]=do_training(m,opt,tloader,vloader,epochs,showoutput,klfactor=0.8,xscale=2.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vloader,m)\n",
    "linear_regression(vloader,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e59c8",
   "metadata": {},
   "source": [
    "RUN the following code to start training the 1D3 model based on the longer PDB list. This is what we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e49bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494431\n",
      "0 0.004567292452008822 0.000503221019814764\n",
      "1 0.00043411648880906 0.0006201102154167494\n",
      "2 0.00030608528809405663 0.0003122270485474996\n",
      "3 0.000245485853365855 0.00021407806148433507\n",
      "4 0.00022640676214073557 0.00024198083102358657\n",
      "5 0.00021964451104217604 0.00020349398799896342\n",
      "6 0.00019842600994722322 0.000188187591427534\n",
      "7 0.00019087604152003755 0.00018646802186578226\n",
      "8 0.00018836516372417914 0.0001857834713849864\n",
      "9 0.00018109120247086793 0.0001623210976907219\n",
      "10 0.00017064490547493452 0.00016033856068947043\n",
      "11 0.00016828449946181502 0.00014855146032869925\n",
      "12 0.00016345099221769225 0.00019600255333639162\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     12\u001b[0m showoutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m [tloss,vloss]\u001b[38;5;241m=\u001b[39m\u001b[43mdo_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtxyzloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvxyzloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshowoutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mklfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mxscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m plot_progress(epochs,tloss,vloss)\n\u001b[1;32m     17\u001b[0m plot_validation(vxyzloader,m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mdo_training\u001b[0;34m(m, opt, tloader, vloader, epochs, output, xscale, yscale, zscale, klfactor)\u001b[0m\n\u001b[1;32m     45\u001b[0m vloss\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(epochs)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 48\u001b[0m     tloss[i] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43myscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43mzscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43mklfactor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     vloss[i] \u001b[38;5;241m=\u001b[39m validate(m,loss_fn,vloader,xscale,yscale,zscale)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (output):\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(m, loss_fn, opt, loader, xscale, yscale, zscale, klfactor)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      8\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m              \u001b[38;5;66;03m# this is where the model is evaluated\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m  loss_fn(output[:,\u001b[38;5;241m0\u001b[39m], label[:,\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39mxscale  \u001b[38;5;66;03m# model loss for x\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(output[:,\u001b[38;5;241m1\u001b[39m], label[:,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m*\u001b[39myscale  \u001b[38;5;66;03m# model loss for y\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mModel1D3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#print(x.size())\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_2(x)) \n\u001b[1;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_f(x)       \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "[tloader,vloader,txyzloader,vxyzloader]= \\\n",
    "    get_loaders('../generatingfeatures/longer_local_cai_aa_capm2opmnpmcpm.dat',0.8,\\\n",
    "                batch_size=512) \n",
    "\n",
    "m = Model1D3()\n",
    "m.apply(m.initialize_weights)\n",
    "m.zero_grad()\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.001, weight_decay=0.000001)\n",
    "\n",
    "epochs=30\n",
    "showoutput=True\n",
    "\n",
    "[tloss,vloss]=do_training(m,opt,txyzloader,vxyzloader,epochs,showoutput,klfactor=2.0,xscale=10.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vxyzloader,m)\n",
    "linear_regression(vxyzloader,m)\n",
    "\n",
    "torch.save(m.state_dict(),\"ca_1d3_predict.dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01581e0f",
   "metadata": {},
   "source": [
    "We can continue with the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "[tloss,vloss]=do_training(m,opt,txyzloader,vxyzloader,epochs,showoutput,klfactor=2.0,xscale=10.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vxyzloader,m)\n",
    "linear_regression(vxyzloader,m)\n",
    "\n",
    "torch.save(m.state_dict(),\"ca_1d3_predict_2.dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a736d",
   "metadata": {},
   "source": [
    "and more training after loading the model again using a smaller learning rate.\n",
    "\n",
    "The following code can be run after stopping and restarting the notebook because it starts from a saved set of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Model1D3()\n",
    "m.load_state_dict(torch.load('ca_1d3_predict_2.dict'))\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.0001, weight_decay=0.000001)\n",
    "\n",
    "[tloader,vloader,txyzloader,vxyzloader]= \\\n",
    "    get_loaders('../generatingfeatures/longer_local_cai_aa_capm2opmnpmcpm.dat',0.8,\\\n",
    "                batch_size=512) \n",
    "\n",
    "showoutput=True\n",
    "epochs=10\n",
    "\n",
    "[tloss,vloss]=do_training(m,opt,txyzloader,vxyzloader,epochs,showoutput,klfactor=0.5,xscale=10.0)\n",
    "\n",
    "plot_progress(epochs,tloss,vloss)\n",
    "plot_validation(vxyzloader,m)\n",
    "linear_regression(vxyzloader,m)\n",
    "\n",
    "torch.save(m.state_dict(),\"ca_1d3_predict_3.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d36983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
